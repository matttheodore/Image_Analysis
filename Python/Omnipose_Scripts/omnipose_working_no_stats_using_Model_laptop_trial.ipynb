{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omnipose Segmentation from ImageJ Macro converted image directories\n",
    "\n",
    "This file is meant to aid in omnipose segmentation in a reproducible and streamlined way to help with automated image analysis especially early QC to adjust experimental and imaging parameters as needed to optimize S/N for the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for all chunks\n",
    "import os\n",
    "import shutil\n",
    "from aicsimageio.readers.ome_tiff_reader import OmeTiffReader\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imsave\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-02 14:06:31,279 [INFO] ** TORCH GPU version installed and working. **\n",
      ">>> GPU activated? True\n"
     ]
    }
   ],
   "source": [
    "# omnipose setup and GPU\n",
    "from cellpose_omni import models, core\n",
    "import torch\n",
    "use_GPU = core.use_gpu()\n",
    "print('>>> GPU activated? {}'.format(use_GPU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from aicsimageio.readers.ome_tiff_reader import OmeTiffReader\n",
    "\n",
    "# Mapping dictionary for renaming channels\n",
    "channel_map = {'Phase': 'phase', 'Far-Red': 'fish', 'DAPI': 'dapi', 'eGFP' : 'gfp'}\n",
    "\n",
    "# Root directory\n",
    "root_dir = r'C:\\Users\\mattt\\Documents\\analyzed_images_holdfoanalysis_laptop\\2022.08.24_LZ22225_pilusrlease_pilusobstruction'  #this would be a directory where your biorep level folder is stored\n",
    "\n",
    "# Navigate through directories to find OME.TIFF files and rename them\n",
    "for biorep_dir in os.listdir(root_dir):\n",
    "    biorep_path = os.path.join(root_dir, biorep_dir)\n",
    "    if os.path.isdir(biorep_path):\n",
    "        for date_strain_dir in os.listdir(biorep_path):\n",
    "            date_strain_path = os.path.join(biorep_path, date_strain_dir)\n",
    "            if os.path.isdir(date_strain_path):\n",
    "                for sub_dir in os.listdir(date_strain_path):\n",
    "                    sub_dir_path = os.path.join(date_strain_path, sub_dir)\n",
    "                    if os.path.isdir(sub_dir_path):\n",
    "                        for img_data_dir in os.listdir(sub_dir_path):\n",
    "                            img_data_path = os.path.join(sub_dir_path, img_data_dir)\n",
    "                            if os.path.isdir(img_data_path):\n",
    "                                for file in os.listdir(img_data_path):\n",
    "                                    if file.endswith('.ome.tiff') or file.endswith('.ome.tif'):\n",
    "                                        file_path = os.path.join(img_data_path, file)\n",
    "                                        \n",
    "                                        # Read the OME.TIFF file to get channel names\n",
    "                                        reader = OmeTiffReader(file_path)\n",
    "                                        ome_metadata = reader.ome_metadata\n",
    "                                        channel_names = [channel.name for channel in ome_metadata.images[0].pixels.channels]\n",
    "                                        \n",
    "                                        # Rename folders and files based on channel names\n",
    "                                        for i, channel_name in enumerate(channel_names):\n",
    "                                            # Map the original channel name to the new name using the channel_map dictionary\n",
    "                                            mapped_name = channel_map.get(channel_name, channel_name)\n",
    "                                            \n",
    "                                            # Create the old and new folder names based on channel index\n",
    "                                            old_folder_name = f\"C{i+1}\"\n",
    "                                            new_folder_name = f\"{mapped_name}\"\n",
    "                                            \n",
    "                                            # Create the full path to the old and new folder names\n",
    "                                            old_folder_path = os.path.join(img_data_path, old_folder_name)\n",
    "                                            new_folder_path = os.path.join(img_data_path, new_folder_name)\n",
    "                                            \n",
    "                                            # If the old folder exists, rename it to the new folder name\n",
    "                                            if os.path.exists(old_folder_path):\n",
    "                                                shutil.move(old_folder_path, new_folder_path)\n",
    "                                            \n",
    "                                            # Rename individual single-page TIFF files inside the new folder\n",
    "                                            for single_tiff in os.listdir(new_folder_path):\n",
    "                                                # Check if the file starts with the old channel name\n",
    "                                                if single_tiff.startswith(f\"C{i+1}\"):\n",
    "                                                    # Create the full path to the old single-page TIFF file\n",
    "                                                    old_single_tiff_path = os.path.join(new_folder_path, single_tiff)\n",
    "                                                    \n",
    "                                                    # Create the new single-page TIFF file name based on mapped channel name\n",
    "                                                    new_single_tiff_name = single_tiff.replace(f\"C{i+1}\", f\"{mapped_name}\")\n",
    "                                                    \n",
    "                                                    # Create the full path to the new single-page TIFF file\n",
    "                                                    new_single_tiff_path = os.path.join(new_folder_path, new_single_tiff_name)\n",
    "                                                    \n",
    "                                                    # Rename the old single-page TIFF file to the new name\n",
    "                                                    shutil.move(old_single_tiff_path, new_single_tiff_path)\n",
    "                                            \n",
    "                                            # Create old and new multi-page TIFF file names based on channel index\n",
    "                                            old_file_name = f\"C{i+1}.tif\"\n",
    "                                            new_file_name = f\"{mapped_name}.tif\"\n",
    "                                            \n",
    "                                            # Create the full path to the old and new multi-page TIFF files\n",
    "                                            old_file_path = os.path.join(img_data_path, old_file_name)\n",
    "                                            new_file_path = os.path.join(img_data_path, new_file_name)\n",
    "                                            \n",
    "                                            # If the old multi-page TIFF file exists, rename it to the new name\n",
    "                                            if os.path.exists(old_file_path):\n",
    "                                                shutil.move(old_file_path, new_file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Omnipose for Segmentation\n",
    "\n",
    "Here is the incorporation into the omnipose script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the images and QC to check images match expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting all the tiff files for omnipose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of images processed: 0\n",
      "No issues found in images.\n"
     ]
    }
   ],
   "source": [
    "from skimage import io  # Importing the io module from skimage for image reading\n",
    "\n",
    "# Initialize an empty list to store the full paths of all phase-MAX_sequence TIFF files.\n",
    "# This list will include both newly renamed and previously renamed phase files.\n",
    "all_phase_max_sequence_files = []\n",
    "\n",
    "# Counter for total images\n",
    "total_images = 0\n",
    "\n",
    "# Counter for images with issues\n",
    "issues_counter = 0\n",
    "\n",
    "# Use os.walk to navigate through the directory tree rooted at root_dir.\n",
    "# os.walk yields a 3-tuple (dirpath, dirnames, filenames) for each directory it visits.\n",
    "# dirpath is the path to the current directory, dirnames is a list of subdirectories in the current directory,\n",
    "# and filenames is a list of filenames in the current directory.\n",
    "\n",
    "# Loop through the directory structure\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for dir in dirs:\n",
    "        if dir == \"phase-MAX_sequence\":\n",
    "            phase_folder_path = os.path.join(root, dir)\n",
    "            for file in os.listdir(phase_folder_path):\n",
    "                if file.endswith(\".tif\"):\n",
    "                    full_file_path = os.path.join(phase_folder_path, file)\n",
    "                    all_phase_max_sequence_files.append(full_file_path)\n",
    "                    \n",
    "                    # Read the image into an array\n",
    "                    img = io.imread(full_file_path)\n",
    "                    \n",
    "                    # Perform quality checks\n",
    "                    shape = img.shape\n",
    "                    dtype = img.dtype\n",
    "                    min_val, max_val = img.min(), img.max()\n",
    "\n",
    "                    # Increment the total_images counter\n",
    "                    total_images += 1\n",
    "\n",
    "                    #quality control checks here\n",
    "                    if shape != (512, 512) or min_val < 3500 or max_val > 35000:\n",
    "                        issues_counter += 1\n",
    "                        print(f\"Warning: Image at {full_file_path} has issues.\")\n",
    "                        print(f\"  - Original image shape: {shape}\")\n",
    "                        print(f\"  - Data type: {dtype}\")\n",
    "                        print(f\"  - Data range: min {min_val}, max {max_val}\")\n",
    "\n",
    "print(f\"\\nTotal number of images processed: {total_images}\")\n",
    "if issues_counter:\n",
    "    print(f\"Number of images with issues: {issues_counter}\")\n",
    "else:\n",
    "    print(\"No issues found in images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA-enabled GPU found. Switching to GPU mode.\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from skimage import img_as_uint \n",
    "import numpy as np\n",
    "from cellpose_omni import models, utils, io as cellpose_io\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "import time\n",
    "from tifffile import TiffFile, imwrite\n",
    "from tifffile import TiffFile, imsave\n",
    "import re\n",
    "\n",
    "# Check for CUDA-enabled GPU availability\n",
    "# Uncomment this block when you want to switch to GPU computation\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability and set the gpu flag\n",
    "if torch.cuda.is_available():\n",
    "    gpu = True\n",
    "    print(\"CUDA-enabled GPU found. Switching to GPU mode.\")\n",
    "else:\n",
    "    gpu = False\n",
    "    print(\"No CUDA-enabled GPU found. Running on CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-02 13:45:36,705 [INFO] ** TORCH GPU version installed and working. **\n",
      "2024-01-02 13:45:36,706 [INFO] >>>> using GPU\n"
     ]
    }
   ],
   "source": [
    "# Define function to create subdirectories\n",
    "def create_sub_dirs(sequence_folder):\n",
    "    sub_dirs = ['masks', 'outlines']\n",
    "    for sub_dir in sub_dirs:\n",
    "        sub_dir_path = os.path.join(sequence_folder, sub_dir)\n",
    "        if not os.path.exists(sub_dir_path):\n",
    "            os.makedirs(sub_dir_path)\n",
    "\n",
    "# Define Function for saving multi-page results\n",
    "def create_output_dirs(output_folder):\n",
    "    sub_dirs = ['cell_only', 'background_only']\n",
    "    for sub_dir in sub_dirs:\n",
    "        sub_dir_path = os.path.join(output_folder, sub_dir)\n",
    "        if not os.path.exists(sub_dir_path):\n",
    "            os.makedirs(sub_dir_path)\n",
    "\n",
    "# Function to extract sequence numbers from filenames\n",
    "def extract_sequence_number(filename):\n",
    "    match = re.search(r'-(\\d{4})\\.tif', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function for Extracting the Multipage Tiff within Directory     \n",
    "def find_multipage_tiff(directory):\n",
    "    current_dir = directory\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    all_files = os.listdir(os.path.dirname(current_dir))\n",
    "    filtered_files = [f for f in all_files if \"LZ222\" in f and \"ome\" not in f]\n",
    "    return os.path.join(parent_dir, filtered_files[0])\n",
    "\n",
    "# Initialize model\n",
    "model_path = r\"C:\\Users\\mattt\\Documents\\Omni\\final_4000_epoch_cellpose_residual_on_style_on_concatenation_off_omni_nclasses_4_omni_retrain_2023_11_01_01_34_28.551587\"\n",
    "model = model = models.CellposeModel(\n",
    "    pretrained_model=model_path, \n",
    "    gpu=gpu, \n",
    "    omni=True,\n",
    "    nchan=2,      # same as training \n",
    "    nclasses=3  # same as training\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# define parameters\n",
    "params = {\n",
    "    'channels': [0,0],  # Segment based on first channel, no second channel\n",
    "    'rescale': None,  # upscale or downscale your images, None = no rescaling\n",
    "    'mask_threshold': -1,  # erode or dilate masks with higher or lower values\n",
    "    'flow_threshold': 0,  # default is .4, but only needed if there are spurious masks to clean up; slows down output\n",
    "    'transparency': True,  # transparency in flow output\n",
    "    'omni': True,  # we can turn off Omnipose mask reconstruction, not advised\n",
    "    'cluster': True,  # use DBSCAN clustering\n",
    "    'resample': True,  # whether or not to run dynamics on rescaled grid or original grid\n",
    "    'verbose': False,  # turn on if you want to see more output\n",
    "    'tile': False,  # average the outputs from flipped (augmented) images; slower, usually not needed\n",
    "    'niter': None,  # None lets Omnipose calculate # of Euler iterations (usually <20) but you can tune it for over/under segmentation\n",
    "    'augment': False,  # Can optionally rotate the image and average outputs, usually not needed\n",
    "    'affinity_seg': False,  # new feature, stay tuned...\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## Segmentation and post-processing\n",
    "for file in sorted(all_phase_max_sequence_files):\n",
    "    sequence_number = extract_sequence_number(os.path.basename(file))\n",
    "    image = imread(file)\n",
    "\n",
    "    try:\n",
    "        # Apply the model\n",
    "        masks, flows, styles = model.eval(image, **params)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping file {file} due to error: {e}\")\n",
    "        continue  # This will skip the rest of the current loop iteration and move to the next file\n",
    "    \n",
    "    # Generate cell-only and background-only images\n",
    "    cell_only_image = image * (masks > 0)\n",
    "    background_only_image = image * (masks == 0)\n",
    "    \n",
    "    label_image = label(masks)\n",
    "\n",
    "    # Create subdirectories for saving within phase-max\n",
    "    directory = os.path.dirname(file)\n",
    "    create_sub_dirs(directory)\n",
    "    filename = os.path.basename(file)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "     # Find the corresponding multi-page TIFF\n",
    "    tiff_path = find_multipage_tiff(os.path.dirname(file)) # * i dont know why this is grabbed here seems out of place \n",
    "    with TiffFile(tiff_path) as tif: \n",
    "        multi_page_tiff = tif.asarray() #read image into a numpy array\n",
    "\n",
    "    # Initialize output folders\n",
    "    output_folder_cell_only = os.path.join(os.path.dirname(tiff_path), 'cell_only')\n",
    "    output_folder_bg_only = os.path.join(os.path.dirname(tiff_path), 'background_only')\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    create_output_dirs(output_folder_cell_only)\n",
    "    create_output_dirs(output_folder_bg_only)\n",
    "    \n",
    "    sequence_number = sequence_number -1\n",
    "\n",
    "# Apply the mask to each channel in each timepoint and Z-plane, just use the current mask \n",
    "    if sequence_number < multi_page_tiff.shape[0]: \n",
    "    \n",
    "        for z in range(multi_page_tiff.shape[1]):\n",
    "            for channel in range(multi_page_tiff.shape[2]):\n",
    "                single_image = multi_page_tiff[sequence_number, z, channel, :, :]\n",
    "                single_image_cells = single_image * (masks > 0)\n",
    "                single_image_background = single_image * (masks == 0)\n",
    "                        \n",
    "                # Generate the output paths\n",
    "                output_cell_only_path = os.path.join(output_folder_cell_only, f\"Time_{sequence_number}_Z_{z}_Channel_{channel}.tif\")\n",
    "                output_bg_only_path = os.path.join(output_folder_bg_only, f\"Time_{sequence_number}_Z_{z}_Channel_{channel}.tif\")\n",
    "                        \n",
    "                # Save the cell-only and background-only images\n",
    "                imsave(output_cell_only_path, single_image_cells)\n",
    "                imsave(output_bg_only_path, single_image_background)\n",
    "    else:\n",
    "            print(f\"Skipping timepoint {adjusted_timepoint} as it is out of bounds.\")\n",
    "\n",
    "    # Modify the output paths\n",
    "    output_cell_only_path = os.path.join(directory, 'cell_only', f\"{base_name}_cell_only.tif\")\n",
    "    output_background_only_path = os.path.join(directory, 'background_only', f\"{base_name}_background_only.tif\")\n",
    "    output_outlines_path = os.path.join(directory, 'outlines', f\"{base_name}_outlines.txt\")\n",
    "    output_mask_path = os.path.join(directory, 'masks', f\"{base_name}_mask.tif\")\n",
    "    \n",
    "    # Save the images and outlines\n",
    "    outlines = utils.outlines_list(masks)\n",
    "    cellpose_io.outlines_to_text(output_outlines_path, outlines)\n",
    "    imsave(output_mask_path, masks.astype(np.uint16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Analysis\n",
    "\n",
    "Now that I have all of the images post mask processing in an organized format I can look into reading them into the memory and performing statistics on them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing Functions for Data Extraction and Metadata Assignments\n",
    "\n",
    "This is where we extract the bckground vs cell only data and assign them to their correct position in the image. This can be very useful to use live/iteratively while imaging to ensure you are taking good reproducible images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import ast \n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def calculate_image_stats(image_path):\n",
    "    # Read the image\n",
    "    image = imread(image_path)\n",
    "\n",
    "    # Check if the image is empty or all zeros\n",
    "    if image.size == 0 or np.all(image == 0):\n",
    "        print(f\"Warning: The image at {image_path} is empty or all zeros.\")\n",
    "        return None  # Return None to indicate that stats could not be calculated\n",
    "\n",
    "    # Filter out the zero pixels\n",
    "    image = image[image > 0]\n",
    "\n",
    "    # Initialize a dictionary to store the statistics\n",
    "    stats_dict = {\n",
    "        'mean': np.mean(image),\n",
    "        'median': np.median(image),\n",
    "        'max': np.max(image),\n",
    "        'min': np.min(image),\n",
    "        'std_dev': np.std(image),\n",
    "        'skewness': scipy.stats.skew(image),\n",
    "        'kurtosis': scipy.stats.kurtosis(image),\n",
    "        'pixel_count': len(image),\n",
    "        'area_covered': len(image) / 262144,\n",
    "        'full_filepath': str(image_path)\n",
    "    }\n",
    "\n",
    "    # Extract metadata from the file path\n",
    "    path_parts = Path(image_path).parts\n",
    "    file_name = Path(image_path).name\n",
    "\n",
    "    # Attempt to extract metadata from the filename\n",
    "    filename_parts = file_name.split('_')\n",
    "    try:\n",
    "        stats_dict['frame'] = filename_parts[-3]\n",
    "        stats_dict['z_stack'] = filename_parts[-2]\n",
    "        stats_dict['channel'] = filename_parts[-1].split('.')[0]\n",
    "    except IndexError:\n",
    "        print(f\"Failed to extract frame, z_stack, channel from {file_name}\")\n",
    "\n",
    "    # Attempt to extract metadata from the directory structure\n",
    "    try:\n",
    "        # Extracting condition, time, and strain from the directory name\n",
    "        # which is third from the last in the path\n",
    "        directory_name = path_parts[-3]\n",
    "        dir_name_parts = directory_name.split('_')\n",
    "        stats_dict['condition'] = dir_name_parts[-1].split('.')[0] if dir_name_parts[-1].split('.')[0].isalpha() else 'unknown'\n",
    "        stats_dict['time'] = dir_name_parts[-2]\n",
    "        stats_dict['strain'] = dir_name_parts[-3]\n",
    "    except IndexError:\n",
    "        print(f\"Failed to extract condition, time, strain from {directory_name}\")\n",
    "\n",
    "    # Extract 'image_type' from the directory name which is second from the last in the path\n",
    "    image_type_directory = path_parts[-2]\n",
    "    if 'background_only' in image_type_directory:\n",
    "        stats_dict['image_type'] = 'background'\n",
    "    elif 'cell_only' in image_type_directory:\n",
    "        stats_dict['image_type'] = 'cell'\n",
    "    else:\n",
    "        stats_dict['image_type'] = 'unknown'\n",
    "\n",
    "    # Extract 'biorep' from the directory name which is further back in the path\n",
    "    biorep = next((part for part in reversed(path_parts) if 'biorep' in part.lower()), None)\n",
    "    if biorep:\n",
    "        biorep_match = re.search(r'biorep(\\d+)', biorep, re.IGNORECASE)\n",
    "        stats_dict['biorep'] = biorep_match.group(1) if biorep_match else 'unknown'\n",
    "    else:\n",
    "        stats_dict['biorep'] = 'unknown'\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "# Function to extract metadata for channel naming\n",
    "def extract_metadata_channel(root_dir):\n",
    "    metadata_list = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.ome.tiff') or file.endswith('.ome.tif'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                reader = OmeTiffReader(full_path)\n",
    "                ome_metadata = reader.ome_metadata\n",
    "                channel_names = [channel.name for channel in ome_metadata.images[0].pixels.channels]\n",
    "                metadata_list.append({'full_filepath': full_path, 'channel_names': channel_names})\n",
    "    return pd.DataFrame(metadata_list)\n",
    "\n",
    "#import re  # Importing the regular expression library\n",
    "\n",
    "def merge_metadata(df, df_metadata, channel_map=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Merge metadata into the main dataframe based on matching 'full_filepath'.\n",
    "    A new column 'signal' is created based on the corresponding 'channel_names' from df_metadata.\n",
    "    Optionally, a channel_map can be provided to rename the channels.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The main dataframe containing various columns including 'full_filepath' and 'channel'.\n",
    "    df_metadata (DataFrame): Metadata dataframe containing 'full_filepath' and 'channel_names'.\n",
    "    channel_map (dict, optional): A dictionary to map original channel names to new names.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize an empty list to hold the new 'signal' column values\n",
    "    signal_list = [None] * len(df)\n",
    "    \n",
    "    # Loop through each row in df\n",
    "    for i, row in df.iterrows():\n",
    "        matched = False  # Variable to indicate if a match was found\n",
    "        # Loop through each row in df_metadata\n",
    "        for j, row_metadata in df_metadata.iterrows():\n",
    "            # Separate the root from the filename using rsplit\n",
    "            root_metadata = row_metadata['full_filepath'].rsplit('\\\\', 1)[0]\n",
    "            \n",
    "            # Create a regex pattern for the root\n",
    "            pattern = re.compile(re.escape(root_metadata))\n",
    "            \n",
    "            # Use regex to find if the root exists in 'full_filepath' of df\n",
    "            if pattern.search(row['full_filepath']):\n",
    "                matched = True  # A match was found, set the variable to True\n",
    "                # Check if the 'channel' value can be converted to an integer\n",
    "                try:\n",
    "                    channel_index = int(row['channel'])\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not convert channel value {row['channel']} to integer at row {i}.\")\n",
    "                    continue  # Skip this row and continue with the next one\n",
    "\n",
    "                # Check the type of channel_names and convert if necessary\n",
    "                if isinstance(row_metadata['channel_names'], str):\n",
    "                    try:\n",
    "                        channel_names = ast.literal_eval(row_metadata['channel_names'])\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Failed to parse channel_names at row {j} in df_metadata: {e}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    channel_names = row_metadata['channel_names']\n",
    "\n",
    "\n",
    "                    # Check if channel_index is within the range of channel_names\n",
    "                    if 0 <= channel_index < len(channel_names):\n",
    "                        signal_name = channel_names[channel_index]\n",
    "                        \n",
    "                        # Apply channel_map if provided\n",
    "                        if channel_map and signal_name in channel_map:\n",
    "                            signal_name = channel_map[signal_name]\n",
    "                        \n",
    "                        signal_list[i] = signal_name\n",
    "                    else:\n",
    "                        print(f\"Warning: channel_index {channel_index} is out of range for channel_names {channel_names} at row {i}.\")\n",
    "                    break  # No need to continue checking for this row, move on to the next one\n",
    "                if not matched:\n",
    "                    print(f\"Warning: No metadata match found for row {i} with filepath {row['full_filepath']}.\")\n",
    "        # Add the new 'signal' column to df\n",
    "        df['signal'] = signal_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics and Metadata Assignment Calculation and loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for tee_stdout and tee_stderr\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def tee_stdout(file_object):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = Tee(sys.stdout, file_object)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "@contextmanager\n",
    "def tee_stderr(file_object):\n",
    "    old_stderr = sys.stderr\n",
    "    sys.stderr = Tee(sys.stderr, file_object)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "# Create the full path by joining root_dir and filename\n",
    "filename = 'warnings_and_output.txt'\n",
    "full_path = os.path.join(root_dir, filename)\n",
    "\n",
    "# File to save warnings\n",
    "with open(full_path, 'w') as f:\n",
    "    with tee_stdout(f), tee_stderr(f):\n",
    "        # Initialize an empty DataFrame to store the image statistics and metadata\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        # Iterate through directories and sub-directories\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".tif\") and ('cell_only' in root or 'background_only' in root):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    stats = calculate_image_stats(file_path)\n",
    "                    \n",
    "                    if stats is not None:  # Only add to DataFrame if stats were successfully calculated\n",
    "                        df = pd.concat([df, pd.DataFrame([stats])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_type</th>\n",
       "      <th>image</th>\n",
       "      <th>channel</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [image_name, image_type, image, channel, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the extracted information\n",
    "data_list = []\n",
    "\n",
    "# Read the text file\n",
    "with open(full_path) as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        # Use regular expressions to match the pattern and extract the relevant parts\n",
    "        match = re.search(r'at (.+\\\\([^\\\\]+)\\\\([^\\\\]+)\\\\([^\\\\]+\\.tif))', line)\n",
    "        if match:\n",
    "            full_path, image_name, image_type, image = match.groups()\n",
    "            channel = re.search(r'Channel_(\\d+)', image).group(1) if re.search(r'Channel_(\\d+)', image) else 'Unknown'\n",
    "            data_list.append([image_name, image_type, image, f\"Channel_{channel}\"])\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_txt = pd.DataFrame(data_list, columns=['image_name', 'image_type', 'image', 'channel'])\n",
    "\n",
    "# Count the occurrences of each unique row based on the group ['image_name', 'image_type', 'channel']\n",
    "df_txt['count'] = df_txt.groupby(['image_name', 'image_type', 'channel'])['image_name'].transform('count')\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_txt_unique = df_txt.drop_duplicates(subset=['image_name', 'image_type', 'channel'])\n",
    "\n",
    "df_txt_unique.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract metadata for channel naming\n",
    "df_metadata = extract_metadata_channel(root_dir)\n",
    "\n",
    "# Merge the metadata into the DataFrame\n",
    "merge_metadata(df, df_metadata, channel_map = channel_map)\n",
    "\n",
    "# Show the DataFrame (For demonstration, will only display the head)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the filename\n",
    "filename = 'df.csv'\n",
    "\n",
    "# Create the full path by joining root_dir and filename\n",
    "full_path = os.path.join(root_dir, filename)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(full_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipose_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
